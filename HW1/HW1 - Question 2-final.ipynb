{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "df_labels = pd.read_csv(\"tweet-train-labels.csv\", delimiter=',', header=None)\n",
    "df_features = pd.read_csv(\"tweet-train-features.csv\", delimiter=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_test = pd.read_csv(\"tweet-test-features.csv\", delimiter=',', header=None)\n",
    "df_labels_test = pd.read_csv(\"tweet-test-labels.csv\", delimiter=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcs = df_labels[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of classes\n",
      "------------------\n",
      "negative \t 0.6054474043715847\n",
      "neutral \t 0.2234460382513661\n",
      "positive \t 0.1711065573770492\n"
     ]
    }
   ],
   "source": [
    "print(\"Ratio of classes\\n------------------\")\n",
    "for vck in vcs.keys():\n",
    "    print(vck, \"\\t\", vcs[vck]/sum(vcs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_labels_num(label_list):\n",
    "    result_list = []\n",
    "    for index, row in label_list.iterrows():\n",
    "        label = row[0]\n",
    "        if label == \"positive\":\n",
    "            result_list.append(1)\n",
    "        elif label == \"neutral\":\n",
    "            result_list.append(0)\n",
    "        else:\n",
    "            result_list.append(-1)\n",
    "    return pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    assert(len(actual) == len(predicted))\n",
    "    for i in range(len(actual)):\n",
    "        if actual.iloc[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_ln(x, minval=0.0000000001):\n",
    "    result = np.log(x.clip(min=minval))\n",
    "    return result\n",
    "\n",
    "import numpy as np\n",
    "class MultinomialNB(object):\n",
    "    def __init__(self, alpha=0):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # X => features\n",
    "        # y => labels\n",
    "        count_vocabs = X.shape[1]\n",
    "        count_sample = X.shape[0]\n",
    "        \n",
    "        # count_vocabs = vocab length\n",
    "        result_list_dict = {}\n",
    "        zipped = pd.concat([X, y], axis=1)\n",
    "        neg_df = zipped[zipped.iloc[:, -1] == -1].iloc[:,0:-1]\n",
    "        neut_df = zipped[zipped.iloc[:, -1] == 0].iloc[:,0:-1]\n",
    "        pos_df = zipped[zipped.iloc[:, -1] == 1].iloc[:,0:-1]\n",
    "                \n",
    "        result_neg = []\n",
    "        for i in range(count_vocabs):\n",
    "            #print(i, \"/\", count_vocabs)\n",
    "            neg_df_trimmed = neg_df.iloc[:, i]\n",
    "            result_neg.append(int(neg_df_trimmed.sum(axis=0)))\n",
    "\n",
    "        result_neg_sum = sum(result_neg)\n",
    "                    \n",
    "        result_neut = []\n",
    "        for i in range(count_vocabs):\n",
    "            #print(i, \"/\", count_vocabs)\n",
    "            neut_df_trimmed = neut_df.iloc[:, i]\n",
    "            result_neut.append(int(neut_df_trimmed.sum(axis=0)))\n",
    "\n",
    "        result_neut_sum = sum(result_neut)\n",
    "         \n",
    "        result_pos = []\n",
    "        for i in range(count_vocabs):\n",
    "            pos_df_trimmed = pos_df.iloc[:, i]\n",
    "            result_pos.append(int(pos_df_trimmed.sum(axis=0)))\n",
    "        \n",
    "        result_pos_sum = sum(result_pos)\n",
    "        \n",
    "        separated = [neg_df, neut_df, pos_df]\n",
    "        self.class_log_prior_ = [np.log(len(i) / count_sample) for i in separated]\n",
    "\n",
    "        results = np.array([result_neg, result_neut, result_pos]) + self.alpha\n",
    "        m = results / ( results.sum(axis=1)[np.newaxis].T + count_vocabs * self.alpha ) \n",
    "\n",
    "        self.feature_log_prob_ = safe_ln(m)\n",
    "        return self\n",
    "    \n",
    "    def predict_log_proba(self, X):\n",
    "        #t0 = time()\n",
    "        res = []\n",
    "        ctr = 1\n",
    "        print(\"Calculating...\\n\")\n",
    "        for index, xrow in X.iterrows():\n",
    "            \n",
    "            feature_log_pr = pd.DataFrame(self.feature_log_prob_)\n",
    "\n",
    "            toSum = [[x * flp[1][index] for index, x in enumerate(xrow) if x != 0] for flp in feature_log_pr.iterrows()]\n",
    "\n",
    "            sumRes = [sum(ts) for ts in toSum]\n",
    "\n",
    "            totalRes = [sr + self.class_log_prior_[index] for index, sr in enumerate(sumRes)]\n",
    "            if((totalRes[1] == totalRes[0] and totalRes[0] >= totalRes[2]) or (totalRes[2] == totalRes[0] and totalRes[0] >= totalRes[1])): #favoring neutral\n",
    "                print(\"\\nHERE\\n\")\n",
    "                totalRes[1] = 1\n",
    "                totalRes[0] = 0\n",
    "                totalRes[2] = 0\n",
    "                \n",
    "            res.append(np.array(totalRes))\n",
    "\n",
    "            print(str(ctr) + \"/\" + str(len(X)), end=\"\\r\", flush=True)\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            ctr += 1\n",
    "        return res\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_log_proba(X), axis=1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction_stats(actual, prediction):\n",
    "    correct = 0\n",
    "    assert(len(actual) == len(prediction))\n",
    "    for i in range(len(actual)):\n",
    "        if actual.iloc[i] == prediction[i]:\n",
    "            correct += 1\n",
    "    print(\"Total predictions made:\\t\", len(actual))\n",
    "    print(\"Correct predictions: \\t\", correct)\n",
    "    print(\"Wrong predictions:   \\t\", len(actual) - correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Dirichlet prior ( First coding problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "df_labels_new = make_labels_num(df_labels)\n",
    "nb.fit(df_features, df_labels_new)\n",
    "df_labels_test = make_labels_num(df_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating...\n",
      "\n",
      "2928/2928\r"
     ]
    }
   ],
   "source": [
    "#for the full test data\n",
    "predicted1 = nb.predict(df_features_test)\n",
    "len(df_features_test)\n",
    "accuracy1 = accuracy_metric(df_labels_test[0], predicted1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  81.86475409836066\n",
      "Total predictions made:\t 2928\n",
      "Correct predictions: \t 2397\n",
      "Wrong predictions:   \t 531\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy1)\n",
    "print_prediction_stats(df_labels_test[0], predicted1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Dirichlet prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating...\n",
      "\n",
      "2928/2928\r"
     ]
    }
   ],
   "source": [
    "# Second question with Alpha value. MAP part\n",
    "nb = MultinomialNB(alpha=1.0)\n",
    "nb.fit(df_features, df_labels_new)\n",
    "predicted2 = nb.predict(df_features_test)\n",
    "len(df_features_test)\n",
    "accuracy2 = accuracy_metric(df_labels_test[0], predicted2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  89.31010928961749\n",
      "Total predictions made:\t 2928\n",
      "Correct predictions: \t 2615\n",
      "Wrong predictions:   \t 313\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy2)\n",
    "print_prediction_stats(df_labels_test[0], predicted2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
